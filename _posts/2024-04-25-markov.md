---
layout: post
title: "Markov Model for Evaluating Soccer Decision Making"
author: "Molly McCann and Dr. Najmeh Salehi" 
---

## Introduction

Markov Decision Processes (MDPs) are mathematical models used in many fields, including inventory control, maintenance, manufacturing and telecommunication, to represent decision-making problems in a stochastic environment [1]. MDPs are named after the Russian mathematician Andrey Markov and are an extension of Markov chains. However, MDPs contain additional components that allow for decision making and rewards for actions. MDPs have various applications in the real world, including finance, agriculture, and sports. In this study, we will explore the application of MDPs in the field of soccer. Soccer is one of the world’s most popular and dynamic team sports. It is characterized by its fluidity, continuous action, and the need for both individual and team strategies. On the other hand, MDPs provide a powerful framework for modeling and optimizing decision-making. MDPs offer a systematic way to capture and address the complexities of the underlying process, making it a suitable tool to analyze soccer. Therefore, MDPs can provide a valuable tool for coaches, players, and researchers seeking to enhance performance and strategic insights in the sport.

## Markov Chains 

A Markov Process, or Markov chain, is a stochastic model used to describe a sequence of events or states where the probability of transitioning from one state to another depends only on the current state and is independent of the past states. This property is called the Markov property or memory-less property. Moreover, a Markov chain consists of the following elements: states, the state space, transition matrix, and the trajectory [2]. Next, we define and discuss each of these components.

### States and State-space

While working with Markov chains, let _{Xt, t = 0, 1, 2,...}_ be a stochastic process. The state of a Markov chain at time _t_ is the value of _Xt_. For example, if _Xt = i_, then the process is said to be in state _i_ at time _t_. The state-space, _S_, is the set of all possible states. To summarize, if _S_ is a countable set, {i,j,k,...}, each _i_∈_S_ is called a state and _S_ is called the state-space. Recalling the Markov property we can say _Xt+1_ depends on the current state _Xt_, but not _Xt−1,...,X1,X0_.

### Transition Matrix 

Before defining what a transition matrix is, there are a few basic definitions that are useful to understanding its components. In Markov chains, vectors consist of positive entries that add up to 1. These vectors are called probability vectors. A square matrix whose rows are probability vectors is called a stochastic matrix. The transition matrix _P_ is a stochastic matrix whose rows represent _Xt_ and whose columns represent _Xt+1._ In other words, assuming that the process is currently in state _i_, a transition matrix shows the probability that the next state will be state _j_. These transition probabilities of the Markov chain are shown as:

Insert equation 1 here 

Thus, the transition matrix itself is _P = \[pij \]_. It lists all the possible states in the state space, _S_. 

Before attempting to create a transition matrix, it is useful to create a diagram that displays the probability of each state given the initial state. As seen in Figure 1, the probability that the process will remain in the same state must be considered, but like all probabilities in a Markov process, its probability does not have to be non-zero. The corresponding transition matrix
for this diagram is displayed in Figure 2. Note that the possible states are all listed across the rows and columns, and the probabilities of transitioning from one state to another is within the matrix. After each iteration of the Markov process, the values of the probabilities in the transition matrix change based on the previous iteration. The following example will apply the newly learned knowledge of states, state-space, and transition matrices to a soccer situation.

Figure 1: An example of a Markov transition diagram with 3 states. 

Figure 2: A transition matrix with 3 possible states. 

#### Example 

The following example of a transition matrix is a small scale application to soccer based on the larger process that will later be modeled using MDPs. Since this application is on a small scale, the field has been broken down into thirds to condense the number of possible states. Figure 3 displays a diagram of the soccer pitch during the first half, operating under the condition that team 1 attempts to score in the goal on the right side of the pitch. Each team will have 4 states which means the resulting transition matrix contains 8 states total. The states are represented by the following:

Figure 3: The zonal and possession breakdown of the pitch. 

* _D1_ - defensive third for team 1
* _C1_ - center third for team 1
* _A1_ - attacking third for team 1
* _S1_ - scored goal for team 1
* _D2_ - defensive third for team 2
* _C2_ - center third for team 2
* _A2_ - attacking third for team 2
* _S2_ - scored goal for team 2

Placing the current state on the rows and the next state as the columns, a transition matrix is formed as a base model of Markov processes in soccer. The values of the probabilities in the matrix are arbitrary with the exception of the first 2 rows. This is because the rules of soccer require that once a goal is scored, the ball returns to the center line for a kick off, starting with the possession of the other team. For example, if team 1 scores a goal (_S1_) then the ball must return to the center line for a team 2 kickoff (_C2_). Thus, the probability that the next state will be _C2_ when the current state is _S1_ is always 1 and vice versa.

There are also a few other values within this transition matrix that are influenced by the physical properties of soccer match. For instance, if the current state is _D1_, meaning team 1 has the ball in their defensive third, then the only way the next state can be _S2_ is if team 1 scores an own goal. This is because team 2 must gain possession of the ball first (which requires another state) before a team member can shoot the ball.

Figure 4: Example of a transtion proabbility matrix for soccer. 

### Trajectory 

A trajectory of a Markov chain is a particular set of values for _Xt_ where _t_= 0,1,2,... The trajectory is generally referred to as _s0,s1,s2,...,_ meaning that _X0 = s0,X1 = s1,X2 = s2,..._ It essentially describes the path that the process takes through the states. The trajectory follows the Markov property because only its most recent point affects what will occur next. In the following example, we apply this term to a soccer oriented situation that will set up the basic comprehension necessary for the later use of MDPs.

#### Example 

Using the matrix in Figure 4, the following example shows a possible trajectory that may occur during in a soccer match. Say the initial state is _C1_, meaning team 1 has the ball in the center third of the field. Then, the ball is intercepted or taken from team 1 but is still within the center third of the field. The state has now transitioned from _C1_ to _C2_. Now that team 2 has the ball, they pass forward to their attacking third. Once in the attacking third team 2 connects a pass with their teammate who then takes a shot, but misses and the ball goes out to the left of the goal. Because the ball went out on the goal line and the last person to touch the ball was a member of team 2, the ball becomes a goal kick for team 1. In other words, this series of events ends in state _D1_. As seen in Figure 5, the entire trajectory for these events that have taken place in this soccer match would follow the trajectory _C1_ to _C2_ to _A2_ to _A2_ to _D1_. Notice that _A2_ is repeated when mapping out the trajectory. This is because there is a state correlated with each action/event that takes place. Thus, when there are two consecutive events within the same third of the field, the trajectory will show the state of every event.

Figure 5: An example trajectory during a small segment of a match.

## Eigenvalues and Eigenvectors 

Eigenvalues are a concept from linear algebra that play a crucial role in various mathematical and scientific applications. Eigenvalues, sometimes referred to as characteristic roots, are a set of scalars that are associated with a linear system of equations [3]. Each eigenvalue has a corresponding eigenvector.

Eigenvalues and eigenvectors have various applications in fields such as physics, engineering,computer science, and more. They are particularly useful in solving systems of linear equations,analyzing dynamic systems, and understanding the behavior of linear transformations. Knowledge of eigenvalues and eigenvectors will assist in the implementation of MDP models on decision making in soccer. Next, we will learn how to solve for eigenvalues and eigenvectors to better understand how they function; however, their computation requires many steps and is better calculated by imputing into computers [3].

### Solving for Eigenvalues and Eigenvectors 

Consider a square matrix _P_ of size _n×n_. An eigenvector of _P_ is a non-zero vector v such that when _P_ operates on **v**, the result is a scaled version of **v**. This scaling factor is known as the eigenvalue associated with **v**. Formally, if **v** is an eigenvector of _P_ with eigenvalue π, then:

Insert equation 2 here. 

Here, π represents the eigenvalue and **v** is the corresponding eigenvector. The process of finding the eigenvalues of a matrix is fairly straightforward. First, find the size of the matrix _P_ for which the eigenvalues and eigenvectors are being solved for. Determine the identity matrix of the same order. Then, using the identity matrix, calculate the matrix _P_− _I_ π, where π is a scalar multiplier. Figure 4 shows the process for finding the matrix _P_ −_I_ π using arbitrary letters as the values inside matrix _P_. After finding the matrix _P_- _I_ π, find its determinant and set the equation equal to zero, i.e. det(_P_ −π _I_) = 0. From the equation created in the previous step, calculate all possible values of π, which are the eigenvalues of the matrix _P_. 

The process of calculating the eigenvectors of a matrix has more complications involved in comparison to finding eigenvalues. Using the eigenvalues found with the above technique, return back to the equation _P_− _I_ π. However, instead of using πas a scalar multiplier, the eigenvalues (all possible values of π) are inserted into the equation. Then, put the matrix or matrices that result from plugging the eigenvalue(s) into reduced row echelon form (RREF). After finding the RREF matrix, reparameterize the values in the matrix according to the free variables. The equation that results from the the reparameterization will lead to the eigenvector that corresponds with that specific eigenvalue. These steps are easier understood using an example in Figure 6.

Figure 6: The process for finding the matrix _P_ −_I_ π

#### Example 

## Markov Decision Processes 

A Markov Decision Process (MDP) provides a mathematical framework for modeling decision making when an outcome is partly random and partly under the control of the decision maker. Finding the solution to an MDP allows for an optimal rule or policy to be derived in order to inform the decision maker on what is the best suited decision to make in order to achieve the desired outcome. MDPs are used for a variety of purposes and disciplines including optimization problems in manufacturing, economics, and sports.

### Markov Reward Processes 

A Markov Reward Process provides a framework for modeling and analyzing dynamic scenarios in which entities transition between different states over time, such as a game character navigating through levels, a robot completing tasks, or the fluctuation of stock market values. The primary objective is to assess the quality of these transitions by evaluating the rewards associated with
them.

A Markov Reward Process can be likened to a soccer player’s journey during a match. In this analogy, the soccer player’s states represent different positions and situations on the field, such as being in possession of the ball, attempting a pass, or defending against an opponent. As the game progresses, the player transitions between these states, making decisions and taking actions. The rewards in this context could symbolize the outcomes of these actions. For instance, successfully scoring a goal or creating a goal-scoring opportunity might yield positive rewards, while losing possession or conceding a goal might lead to negative rewards. 

Analyzing the player’s performance within the Markov Reward Process framework helps assess how effective their movements and decisions are during the game, shedding light on the overall quality of their play and strategic choices.

Let’s define the following notations:
States: A set of states _S= {S1,S2,...,Sn}_
Transition Probabilities: The probability of transitioning from state _Si_ to state _Sj_ and denoted by, _Pij_.
Immediate Rewards: The immediate reward upon entering state _Si_ and denoted by, _Ri_.
Discount Factor: A discount factor for future rewards and denoted by, γ .

#### Transition Probability Matrix 



#### Expected Total Reward 

### Markov Decision Processes 

## MDP Applications in Soccer 

### Modeling Assumptions 

### States and Field Breakdown 

### Defining Actions 

#### Dribble 

#### Short Pass 

#### Mid Pass 

#### Long Pass (Long Ball)

#### Short Cross 

#### Mid Cross 

#### Short Out 

#### Mid Out 

#### Shot

### Transition Probabilities 

#### Defensive Third for Team 1 (D1)

#### Central Third for Team 1 (C1)

#### Attacking Third for Team 1 (A1) 

#### Defensive Third for Team 2 (D2)

#### Central Third for Team 2 (C2)

#### Attacking Third for Team 2 (A2) 

### Rewards 

## Iterative Solution Algorithm 

## Problem Results 

## References 

[1] Henk C Tijms. _A first course in stochastic models._ John Wiley and sons, 2003.

[2] J. R. Norris. _Markov Chains_. Cambridge University Press, 1998.

[3] Herv´e Abdi. The eigen-decomposition: Eigenvalues and eigenvectors. _Encyclopedia of measurement and statistics,_ pages 304–308, 2007.
